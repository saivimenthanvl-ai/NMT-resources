# (NMT) Agent for Lowâ€‘Resource Languages

<p align="center">
  <img alt="NMT" src="https://img.shields.io/badge/NMT-Low%20Resource-blue" />
  <img alt="Python" src="https://img.shields.io/badge/Python-3.10+-3776AB?logo=python&logoColor=white" />
  <img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-2.x-EE4C2C?logo=pytorch&logoColor=white" />
  <img alt="Transformers" src="https://img.shields.io/badge/HF-Transformers-FFD21E?logo=huggingface&logoColor=black" />
  <img alt="CUDA" src="https://img.shields.io/badge/CUDA-ready-76B900?logo=nvidia&logoColor=white" />
  <img alt="License" src="https://img.shields.io/badge/License-MIT-green" />
  <img alt="PRs" src="https://img.shields.io/badge/PRs-welcome-brightgreen" />
</p>

> ğŸ—£ï¸ **Goal**: A productionâ€‘leaning Neural Machine Translation (NMT) agent that **prioritizes lowâ€‘resource languages (LRLs)** through smart data curation, transfer learning, fewâ€‘shot prompting, and quality estimation.

---

## ğŸ”¥ Highlights (Recruiterâ€‘friendly)

* ğŸ“ˆ **SOTAâ€‘inspired pipeline**: Encoderâ€‘decoder Transformers (Marian/ByT5/mBART), adapters (LoRA/PEFT), and retrievalâ€‘augmented glossaries.
* ğŸ§  **Agentic loop**: Translate â†’ Assess with QE â†’ Selfâ€‘refine â†’ Finalize.
* ğŸŒ **LRLâ€‘first design**: Tokenization, domain adaptation, and script handling for Indic and other LRLs.
* ğŸ›¡ï¸ **Responsible AI**: Bias, toxicity/regard checks, and cultural sensitivity prompts bakedâ€‘in.
* âš™ï¸ **Deployable**: CLI + REST stub, batch mode, and reproducible training.

---

## ğŸ§© Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Input   â”‚ â”€â–¶  â”‚  Translatorâ”‚ â”€â–¶  â”‚  QE/COMET  â”‚ â”€â–¶  â”‚ Selfâ€‘Refine â”‚
â”‚ (src.txt) â”‚     â”‚ (HF model) â”‚     â”‚  + Heur.   â”‚     â”‚ (prompting) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
                                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                     â”‚  Final Translation â”‚
                                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Modules**

* `translator_hf.py` â€” wraps Hugging Face models (Marian/mBART/ByT5/Qwenâ€‘VLâ€‘textâ€‘only) with LoRA/PEFT hooks.
* `qe_comet.py` â€” quality estimation via COMET/BLEURT + regex/domain heuristics.
* `tm_glossary.py` â€” terminology memory + dictionary constraints.
* `app.py` â€” CLI/REST entrypoint and agentic loop orchestration.

---

## ğŸ“¦ Quick Start

### 1) Setup

```bash
# clone
git clone https://github.com/<yourâ€‘id>/(NMT)_Agent_for_Low_Resource_Languages.git
cd (NMT)_Agent_for_Low_Resource_Languages

# env
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -U pip wheel
pip install -r requirements.txt
```

### 2) Minimal Inference

```bash
python app.py \
  --src_path data/sample.src.txt \
  --out_path runs/sample.hyp.txt \
  --model_name Helsinki-NLP/opus-mt-en-ta \
  --qe comet \
  --refine 1
```

### 3) Training (Adapter/LoRA)

```bash
python translator_hf.py train \
  --base_model facebook/mbart-large-50 \
  --train_file data/train.jsonl \
  --valid_file data/valid.jsonl \
  --target_lang ta \
  --peft lora \
  --epochs 3 --lr 2e-5 --batch_size 8
```

---

## ğŸ—‚ï¸ Repository Layout

```
.
â”œâ”€â”€ app.py                 # CLI/REST agent
â”œâ”€â”€ translator_hf.py       # HF models + PEFT/LoRA
â”œâ”€â”€ qe_comet.py            # COMET/BLEURT/heuristics
â”œâ”€â”€ tm_glossary.py         # term memory + constraints
â”œâ”€â”€ data/                  # samples + scripts to build datasets
â”œâ”€â”€ configs/               # yaml configs per language/domain
â”œâ”€â”€ runs/                  # outputs, logs, checkpoints
â”œâ”€â”€ notebooks/             # analysis & experiments
â””â”€â”€ README.md
```

---

## ğŸ“š Data & Tokenization (LRLâ€‘centric)

* **Sources**: OPUS, JW300, FLORESâ€‘200, Wiki, domain corpora (health, agri, legal).
* **Cleaning**: deâ€‘dup, languageâ€‘ID (fastText), length/ratio filters, Unicode normalization, punctuation/script checks.
* **Tokenization**: SentencePiece (unigram/BPE) with shared vocab, fallback byteâ€‘level for noisy LRLs.
* **Terminology**: glossary CSVs; enforce via constrained decoding or postâ€‘edit rules.

> ğŸ’¡ *Tip:* Start with multilingual base (mBARTâ€‘50), then **adapterâ€‘tune** per LRL domain to avoid catastrophic forgetting.

---

## ğŸ§ª Evaluation

* **Automatic**: BLEU, chrF, COMET (QE), TER.
* **Human**: adequacy/fluency/terminology accuracy (Likert 1â€“5), cultural fit checklist.
* **Bias/Safety**: regard sentiment, stereotype probes (StereoSetâ€‘style), toxicity lexicons.

```bash
python qe_comet.py eval \
  --src data/test.src.txt --ref data/test.ref.txt --hyp runs/sample.hyp.txt \
  --metrics bleu chrf comet ter
```

---

## ğŸ›¡ï¸ Responsible AI & Safety

* âš–ï¸ **Bias**: assess with stereotype probes, measure **regard**, apply promptâ€‘level counterâ€‘bias patterns.
* ğŸ” **Privacy**: strip PII; optional hashing for IDs.
* ğŸ§¯ **Hallucination control**: domain constraints, glossary locking, QEâ€‘gated selfâ€‘refine.
* ğŸŒ **Cultural sensitivity**: reviewer checklist; avoid literalism when idioms exist.

---

## ğŸš€ Deployment

* **Batch**: process files via CLI (CPU/GPU).
* **Service**: `uvicorn app:api` (FastAPI stub) for HTTP translation.
* **Artifacts**: save `tokenizer.json`, `adapter.safetensors`, `config.yaml` for reproducibility.

---

## ğŸ—ºï¸ Roadmap

* [ ] Better COMETâ€‘QE finetuning for LRLs
* [ ] RAGâ€‘style context retrieval for domain phrases
* [ ] Mixedâ€‘precision + int8 quantization for edge GPUs
* [ ] Activeâ€‘learning loop with human feedback

---

## ğŸ‘©â€ğŸ’» Example Config (YAML)

```yaml
model: facebook/mbart-large-50
lang_pair: en-ta
peft:
  type: lora
  r: 16
  alpha: 32
  dropout: 0.05
train:
  epochs: 3
  batch_size: 8
  lr: 2.0e-5
  max_len: 256
```

---

## ğŸ“ Citation

If you use this repo, please cite or reference it in your work.

```bibtex
@software{nmt_agent_lrl,
  title  = {An Agentic NMT Pipeline for Low-Resource Languages},
  author = {Mieradilijiang Maimaiti, Yang Liu, Huanbo Luan, Maosong Sun},
  year   = {2025},
  url    = {https://github.com/saivimenthanvl-ai/(NMT)_Agent_for_Low_Resource_Languages}
}
```

---

## ğŸ¤ Contributing

PRs are welcome! Please open an issue with details on language/domain and attach small reproducible samples.

---

## ğŸ“„ License

MIT â€” see `LICENSE`.

---

### ğŸ¯ Recruiter Notes

* Built for **Indic LRLs** (e.g., Tamil, English) with **adapterâ€‘tuning** for costâ€‘efficient training.
* Demonstrates **endâ€‘toâ€‘end ownership**: data â†’ modeling â†’ evaluation â†’ deployment â†’ safety.
* Clear **ablation hooks** (with/without glossary/QE/refinement) for researchâ€‘ready results.
